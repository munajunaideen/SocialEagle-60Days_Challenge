{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92dd45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ad52ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (1.0.5)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (0.4.43)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-community) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: anyio in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: certifi in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b250ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c08a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Check if notes.txt exists\n",
    "if os.path.exists('notes.txt'):\n",
    "    test = TextLoader('notes.txt')\n",
    "    docs = test.load()\n",
    "    print(f'Loaded {len(docs)} documents')\n",
    "else:\n",
    "    print('Warning: notes.txt not found in current directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee6c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (1.26.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67db5c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing PyMuPDFLoader: Error importing numpy: you should not try to import numpy from\n",
      "        its source directory; please exit the numpy source tree, and relaunch\n",
      "        your python interpreter from there.\n",
      "Make sure pymupdf is installed and numpy is compatible with your Python version\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from langchain_community.document_loaders import PyMuPDFLoader\n",
    "    print('PyMuPDFLoader imported successfully')\n",
    "except ImportError as e:\n",
    "    print(f'Error importing PyMuPDFLoader: {e}')\n",
    "    print('Make sure pymupdf is installed and numpy is compatible with your Python version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f61ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is ready. Re-run cell 6 (PyMuPDFLoader) if needed.\n"
     ]
    }
   ],
   "source": [
    "# Kernel has been restarted with compatible numpy version (2.2.6)\n",
    "print('Notebook is ready. Re-run cell 6 (PyMuPDFLoader) if needed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acea7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c50ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\n",
      "   ---------------------------------------- 3/3 [bs4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.2 bs4-0.0.2 soupsieve-2.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc5c76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers\\n\\n\\n\\n\\n\\n\\n\\n      \\n      Skip to main content\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n          Machine Learning\\n        \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어\\n\\n\\n\\n\\nSign in\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n    \\n\\n\\n\\n    Resources\\n  \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n          Machine Learning\\n        \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Home\\n   \\n\\n\\n\\n\\n\\n      Resources\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nIntro to LLMs\\nML & AI Basics\\nPrompt Engineering\\nSecure AI Framework\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n        Home\\n      \\n  \\n\\n\\n\\n\\n    \\n        Products\\n      \\n  \\n\\n\\n\\n\\n    \\n        Machine Learning\\n      \\n  \\n\\n\\n\\n\\n    \\n        Resources\\n      \\n  \\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\\n  \\n  \\n\\n\\n      Introduction to Large Language Models\\n\\n\\n      \\n      Stay organized with collections\\n    \\n\\n      \\n      Save and categorize content based on your preferences.\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI-generated Key Takeaways\\n\\n\\n\\n\\n\\noutlined_flag\\n\\n\\n\\n\\nLanguage models predict and generate text by estimating the probability of a token or sequence of tokens occurring within a longer sequence, useful for tasks like text generation and translation.\\n\\n\\nLarge language models (LLMs) are advanced language models with vast parameters and datasets, enabling them to process longer text sequences and perform complex tasks like summarization and question answering.\\n\\n\\nTransformers are a key architecture in LLMs, utilizing attention mechanisms to focus on important input parts and enhance processing efficiency.\\n\\n\\nLLMs have various applications, including text generation, translation, sentiment analysis, and code generation, but also present considerations such as cost, bias, and ethical implications.\\n\\n\\n\\n\\nNew to language models or large language models? Check out the resources below.\\nEstimated Read Time: 20 minutes\\n\\nLearning objectives:\\n\\nDefine language models and large language models (LLMs).\\n\\nDefine key LLM concepts, including Transformers and self-attention.\\n\\nDescribe the costs and benefits of LLMs, along with common use cases.\\n\\n\\n\\nWhat is a language model?\\nA language model is a machine learning\\nmodel\\nthat aims to predict and generate plausible language. Autocomplete is a\\nlanguage model, for example.\\nThese models work by estimating the probability of a\\ntoken or\\nsequence of tokens occurring within a longer sequence of tokens. Consider the\\nfollowing sentence:\\nWhen I hear rain on my roof, I _______ in my kitchen.\\n\\nIf you assume that a token is a word, then a language model determines the\\nprobabilities of different words or sequences of words to replace that\\nunderscore. For example, a language model might determine the following\\nprobabilities:\\ncook soup 9.4%\\nwarm up a kettle 5.2%\\ncower 3.6%\\nnap 2.5%\\nrelax 2.2%\\n...\\n\\nA \"sequence of tokens\" could be an entire sentence or a series of sentences.\\nThat is, a language model could calculate the likelihood of different entire\\nsentences or blocks of text.\\nEstimating the probability of what comes next in a sequence is useful for all\\nkinds of things: generating text, translating languages, and answering\\nquestions, to name a few.\\nWhat is a large language model?\\nModeling human language at scale is a highly complex and resource-intensive\\nendeavor. The path to reaching the current capabilities of language models and\\nlarge language models has spanned several decades.\\nAs models are built bigger and bigger, their complexity and efficacy increases.\\nEarly language models could predict the probability of a single word; modern\\nlarge language models can predict the probability of sentences, paragraphs, or\\neven entire documents.\\nThe size and capability of language models has exploded over the last\\nfew years as computer memory, dataset size, and processing power increases, and\\nmore effective techniques for modeling longer text sequences are developed.\\nHow large is large?\\nThe definition is fuzzy, but \"large\" has been used to describe BERT (110M\\nparameters) as well as PaLM 2 (up to 340B parameters).\\nParameters\\nare the\\nweights\\nthe model learned during training, used to predict the next token in the\\nsequence. \"Large\" can refer either to the number of parameters in the model, or\\nsometimes the number of words in the dataset.\\nTransformers\\nA key development in language modeling was the introduction in 2017 of\\nTransformers, an architecture designed around the idea of\\nattention.\\nThis made it possible to process longer sequences by focusing on the most\\nimportant part of the input, solving memory issues encountered in earlier\\nmodels.\\nTransformers are the state-of-the-art architecture for a wide variety of\\nlanguage model applications, such as translators.\\nIf the input is \"I am a good dog.\", a Transformer-based translator\\ntransforms that input into the output \"Je suis un bon chien.\", which is the\\nsame sentence translated into French.\\nFull Transformers consist of an\\nencoder and a\\ndecoder. An\\nencoder converts input text into an intermediate representation, and a decoder\\nconverts that intermediate representation into useful text.\\nSelf-attention\\nTransformers rely heavily on a concept called self-attention. The self part of\\nself-attention refers to the \"egocentric\" focus of each token in a corpus.\\nEffectively, on behalf of each token of input, self-attention asks, \"How much\\ndoes every other token of input matter to me?\" To simplify matters, let\\'s\\nassume that each token is a word and the complete context is a single\\nsentence. Consider the following sentence:\\n\\nThe animal didn\\'t cross the street because it was too tired.\\n\\nThere are 11 words in the preceding sentence, so each of the 11 words is paying\\nattention to the other ten, wondering how much each of those ten words matters\\nto them. For example, notice that the sentence contains the pronoun it.\\nPronouns are often ambiguous. The pronoun it always refers to a recent noun,\\nbut in the example sentence, which recent noun does it refer to: the animal\\nor the street?\\nThe self-attention mechanism determines the relevance of each nearby word to\\nthe pronoun it.\\nWhat are some use cases for LLMs?\\nLLMs are highly effective at the task they were built for, which is generating\\nthe most plausible text in response to an input. They are even beginning to show\\nstrong performance on other tasks; for example, summarization, question\\nanswering, and text classification. These are called\\nemergent abilities. LLMs can even\\nsolve some math problems and write code (though it\\'s advisable to check their\\nwork).\\nLLMs are excellent at mimicking human speech patterns. Among other things,\\nthey\\'re great at combining information with different styles and tones.\\nHowever, LLMs can be components of models that do more than just\\ngenerate text. Recent LLMs have been used to build sentiment detectors,\\ntoxicity classifiers, and generate image captions.\\nLLM Considerations\\nModels this large are not without their drawbacks.\\nThe largest LLMs are expensive. They can take months to train, and as a result\\nconsume lots of resources.\\nThey can also usually be repurposed for other tasks, a valuable silver lining.\\nTraining models with upwards of a trillion parameters\\ncreates engineering challenges. Special infrastructure and programming\\ntechniques are required to coordinate the flow to the chips and back again.\\nThere are ways to mitigate the costs of these large models. Two approaches are\\noffline inference\\nand\\ndistillation.\\nBias can be a problem in very large models and should be considered in training\\nand deployment.\\nAs these models are trained on human language, this can introduce numerous \\npotential ethical issues, including the misuse of language, and bias in race,\\ngender, religion, and more.\\nIt should be clear that as these models continue to get bigger and perform\\nbetter, there is  continuing need to be diligent about understanding and\\nmitigating their drawbacks. Learn more about Google\\'s approach to\\nresponsible AI.\\nLearn more about LLMs\\nInterested in a more in-depth introduction to large language models? Check\\nout the new Large language models module\\nin Machine Learning Crash Course.\\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\\n  \\n  \\n\\n\\n\\n\\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\\nLast updated 2025-08-25 UTC.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConnect\\n\\n\\n\\n            \\n          \\n            Blog\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Bluesky\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Instagram\\n          \\n          \\n\\n\\n\\n            \\n          \\n            LinkedIn\\n          \\n          \\n\\n\\n\\n            \\n          \\n            X (Twitter)\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            YouTube\\n          \\n          \\n\\n\\n\\n\\nPrograms\\n\\n\\n\\n            \\n          \\n            Google Developer Program\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Developer Groups\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Developer Experts\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Accelerators\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            Google Cloud & NVIDIA\\n          \\n          \\n\\n\\n\\n\\nDeveloper consoles\\n\\n\\n\\n            \\n          \\n            Google API Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Cloud Platform Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Play Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Firebase Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Actions on Google Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Cast SDK Developer Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Chrome Web Store Dashboard\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            Google Home Developer Console\\n          \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Android\\n        \\n\\n\\n\\n          Chrome\\n        \\n\\n\\n\\n          Firebase\\n        \\n\\n\\n\\n          Google Cloud Platform\\n        \\n\\n\\n\\n          Google AI\\n        \\n\\n\\n\\n          All products\\n        \\n\\n\\n\\n\\n\\n\\n\\n          Terms\\n        \\n\\n\\n\\n          Privacy\\n        \\n\\n\\n\\n          Manage cookies\\n        \\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = WebBaseLoader(web_path=\"https://developers.google.com/machine-learning/resources/intro-llms\")\n",
    "test3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860cd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8430839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from arxiv) (2.32.5)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.10.5)\n",
      "Downloading arxiv-2.3.1-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6105 sha256=ba3aea168cea2817e57839955ef05f34ff34db37e752f6c8ea4061bae5b0101e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "\n",
      "   ---------------------------------------- 3/3 [arxiv]\n",
      "\n",
      "Successfully installed arxiv-2.3.1 feedparser-6.0.12 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc52a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2025-11-17', 'Title': 'Phase I Distribution-Free Control Charts for Individual Observations Using Runs and Patterns', 'Authors': 'Tung-Lung Wu', 'Summary': 'Phase I distribution-free runs- and patterns-type control charts are proposed for monitoring the unknown target value (or location parameter) for both continuous and discrete individual observations. Our approach maintains the nominal in-control signal probability at a prescribed level by employing the finite Markov chain imbedding technique combined with random permutation and conditioning arguments. To elucidate the methodology, we examine two popular runs- and patterns-type statistics: the number of success runs and the scan statistic. Numerical results indicate that the performance of our proposed control charts is comparable to that of existing Phase I nonparametric control charts for individual observations.'}, page_content='Statistical Papers manuscript No.\\n(will be inserted by the editor)\\nPhase I Distribution-Free Control Charts for\\nIndividual Observations Using Runs and Patterns\\nTung-Lung Wu\\nReceived: date / Accepted: date\\nAbstract Phase I distribution-free runs- and patterns-type control charts are\\nproposed for monitoring the unknown target value (or location parameter) for\\nboth continuous and discrete individual observations. Our approach maintains\\nthe nominal in-control signal probability at a prescribed level by employing the\\nﬁnite Markov chain imbedding technique combined with random permutation\\nand conditioning arguments. To elucidate the methodology, we examine two\\npopular runs- and patterns-type statistics: the number of success runs and the\\nscan statistic. Numerical results indicate that the performance of our proposed\\ncontrol charts is comparable to that of existing Phase I nonparametric control\\ncharts for individual observations.\\nKeywords Distribution-free · control charts · runs and patterns · number of\\nsuccess runs · scan statistic · ﬁnite Markov chain imbedding\\nMathematics Subject Classiﬁcation (2010) 62P30\\n1 Introduction\\nStatistical process control (SPC) is an eﬀective technique for monitoring the\\ncharacteristics of a process. In a typical control chart application, the analy-\\nsis is divided into two phases: Phase I and Phase II. Phase I control charts\\nare employed to verify that the process is in control (IC), based on a retro-\\nspective analysis of historical data to assess process stability. Once stability\\nis conﬁrmed, the parameter estimates derived from Phase I data are used to\\nTung-Lung Wu\\nDepartment of Mathematics and Statistics, Mississippi State University, MS 39759, United\\nStates\\nTel.: 662-325-3414\\nFax: 662-325-0005\\nE-mail: tw1475@msstate.edu\\narXiv:2511.13672v1  [stat.AP]  17 Nov 2025\\n2\\nTung-Lung Wu\\nestablish a Phase II control chart. The primary objective of the Phase II con-\\ntrol chart is to detect any process changes, such as shifts in location or scale,\\nas quickly as possible.\\nIn a typical control chart, the plotted data points are assumed to follow a\\nparametric distribution F0, such as the normal distribution. A challenge arises\\nwhen the data are not normally distributed. If the distributional assumption is\\nnot satisﬁed, the promised characteristics of the control chart, such as the IC\\naverage run length (ARL) or the false alarm probability (FAP), can no longer\\nbe considered reliable. In other words, these control charts are not robust to\\nsmall deviations from the assumed underlying distribution. Consequently, nu-\\nmerous distribution-free or nonparametric control charts have been developed\\nover the past two decades; a comprehensive review of nonparametric control\\ncharts is provided by Chakraborti et al (2001).\\nIn the past decade, nonparametric or distribution-free control charts have\\nreceived considerable attention in quality and process control. The mainstream\\napproaches include rank-based control charts (Zhou et al 2009; Hawkins and Deng\\n2010) and likelihood-based control charts (Ning et al 2015). Typically, there\\nare three types of control charts: exponentially weighted moving average (EWMA),\\ncumulative sum (CUSUM), and Shewhart control charts. Conventional She-\\nwhart control charts can be enhanced by incorporating supplemental runs\\nrules. EWMA and CUSUM charts are particularly popular because they are\\nmore eﬀective at detecting small mean shifts. For example, nonparametric\\nEWMA control charts have been developed in the works of Amin and Searcy\\n(1991) and Zou and Tsung (2010), while nonparametric CUSUM control charts\\nhave been investigated by Chatterjee and Qiu (2009), Chowdhury et al (2015),\\nand Li et al (2013).\\nThe simplicity, ease of implementation, and tractability of run-length char-\\nacteristics make Shewhart-type charts highly attractive. Runs rules have been\\napplied to enhance Shewhart control charts for detecting small mean shifts,\\nsimilar to their application in scan statistics (Shmueli and Cohen 2003). How-\\never, the performance of these charts is not guaranteed when normality or spe-\\nciﬁc parametric models are assumed (Champ and Woodall 1987; Koutras et al\\n2007). To overcome these limitations, Chakraborti et al (2009) proposed a\\nPhase-II nonparametric control chart based on precedence statistics with runs\\nrules for monitoring an unknown location parameter. Their method signals\\nwhen two consecutive plotting points—such as the median—fall outside the\\ncontrol limits. Another approach, based on the Wilcoxon signed-rank statistic\\ncombined with runs-type rules, was presented by Chakraborti and Eryilmaz\\n(2007); however, this statistic is applicable only when the median is known.\\nFinally, Balakrishnan et al (2010) introduced a general class of nonparametric\\ncontrol charts based on order statistics, providing a detailed review of the ﬁeld.\\nThe Phase-I control chart is crucial in process monitoring, as the success of\\nthe Phase-II control chart depends on the eﬀectiveness of the Phase-I control\\nchart. However, there are limited results in the literature regarding Phase-I\\ncontrol charts. Over the last two decades, most distribution-free or nonpara-\\nmetric control charts have been designed for Phase II. Consequently, there is\\nTitle Suppressed Due to Excessive Length\\n3\\na need for new Phase-I control charts. In addition, nonparametric Shewhart\\ncontrol charts with runs rules appear to be underdeveloped relative to non-\\nparametric EWMA and CUSUM control charts. One likely reason is the lack\\nof methods for handling general runs without any distributional assumptions.\\nTo bridge these gaps, we develop Phase-I distribution-free control charts based\\non runs and patterns.\\nRecent research has emphasized the distinct challenges of Phase I analy-\\nsis, including the treatment of retrospective data, estimation uncertainty, sub-\\ngroup formation, and distributional assumptions. In particular, Jones-Farmer et al\\n(2009, 2014) and Capizzi (2015) have provided valuable frameworks for ad-\\ndressing such issues and for integrating estimation and diagnostic considera-\\ntions into Phase I charting. Our proposed runs- and scan-based control charts\\ncomplement this literature by providing exact distribution-free methods for\\nindividual observations. In contrast to existing Phase I approaches that rely\\non estimated parameters or subgroup summaries, the present work establishes\\na model-free foundation using conditional distributions of runs and scan statis-\\ntics derived via the ﬁnite Markov chain imbedding technique.\\nThe ﬁnite Markov chain imbedding (FMCI) technique has been employed\\nto evaluate the exact characteristics of the run-length distribution, such as\\nthe mean and standard deviation, of various Phase II control charts, including\\nShewhart, EWMA, and CUSUM charts under the assumption of normality\\n(Fu et al 2002, 2003). In this work, we study Phase I distribution-free runs-\\nand patterns-type control charts, with the conditional distributions of runs\\nand patterns derived exactly using the FMCI technique. In this framework,\\nBernoulli trials, given the total number of successes, are treated as random per-\\nmutations. This approach coincides with other works which have shown that\\nthe conditional distributions of the proposed monitoring statistics are inde-\\npendent of unknown parameters and the underlying distributions (Chen et al\\n2016).\\nMuch of the existing literature on distribution-free runs-type control charts\\nis either limited to speciﬁc run statistics or not entirely distribution-free. In\\nthis study, we propose a general framework for constructing distribution-free\\nruns- and patterns-type control charts for individual observations.\\nThe remainder of this manuscript is organized as follows. Section 2 intro-\\nduces the conditional distributions of the number of success runs, Rn, and the\\nscan statistic, Sn(r). Section 3 outlines the charting procedures for scan-based\\nand runs-based control charts. Section 4 presents the numerical results. Sec-\\ntion 5 demonstrates an application using piston ring data. Finally, Section 6\\noﬀers concluding remarks and discussion.\\n2 Conditional distributions of Rn and Sn(r) given the number of\\nsuccesses\\nLet X1, . . . , Xn be a sequence of independent and identically distributed (i.i.d.)\\nBernoulli trials with success probability p = Pr(X1 = 1), and let N1 denote the\\n4\\nTung-Lung Wu\\nnumber of successes in the sequence. This section provides a detailed account\\nof the ﬁnite Markov chain imbedding (FMCI) technique, which is employed to\\nderive the exact conditional distributions of the number of success runs and\\nscan statistics given the number of successes.\\n2.1 Number of success runs\\nA success run is deﬁned as a maximal consecutive subsequence of 1s, that is, a\\nsequence (Xj, Xj+1, . . . , Xk) = (1, 1, . . . , 1) such that Xj−1 = 0 (or j = 1) and\\nXk+1 = 0 (or k = n). The total number of such success runs in the sequence\\nis denoted by Rn.\\nThe exact unconditional distribution of the number of success runs was\\nderived by Fu and Koutras (1994). The conditional distribution of the number\\nof success runs has been discussed in several studies, including Lou (1997) and\\nthe book by Fu and Lou (2003). In this work, we adopt the method presented\\nby Lou (1997), as it leads to a smaller state space for the imbedded Markov\\nchain. Let\\nP =\\n(\\nπ = (π1, . . . , πn) : πi ∈{0, 1},\\nn\\nX\\ni=1\\nπi = n1\\n)\\ndenote the set of random permutations consisting of n1 ones and n2 = n −n1\\nzeros. The conditional distribution of Rn, given a total of n1 successes in a\\nsequence of n Bernoulli trials, is equivalent to the distribution of Rn in a\\nrandom permutation π ∈P.\\nTo construct the imbedded Markov chain, we sequentially and randomly\\ninsert n2 zeros into an initial row of n1 ones, one at a time, over n2 steps.\\nAt each insertion step, we record the number of success runs. The Markov\\nchain {Yt} is thus deﬁned on the state space Ω1 = {1, 2, . . ., d}, where d =\\nmin(n1, n2 + 1) is the maximum possible number of success runs. During the\\ninsertion process, a new success run is created if a 0 is inserted between two\\nconsecutive 1s; otherwise, the number of runs remains unchanged.\\nLet M1\\nt denote the transition matrix at time t, for t = 1, 2, . . . , n2. The\\ntransition probabilities are deﬁned as\\npij(t) = Pr(Yt = j | Yt−1 = i)\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nn1−i\\nn1+t,\\nif j = i + 1, 1 ≤i ≤t,\\nt+i\\nn1+t,\\nif j = i, 1 ≤i ≤t,\\n1,\\nif j = i ≥t + 1, i ≤d −1, or j = i = d,\\n0,\\notherwise.\\n(1)\\nAccording to Theorem 2.1 in Fu and Lou (2003), the conditional random\\nvariable Rn is ﬁnite Markov chain imbeddable. The conditional distribution\\nTitle Suppressed Due to Excessive Length\\n5\\nof Rn, given N1 = n1, is then expressed as\\nPr(Rn = r | N1 = n1) = ξ0\\nn2\\nY\\nt=1\\nM1\\nter,\\n(2)\\nwhere ξ0 = (1, 0, . . . , 0) is the initial distribution vector and er is a column\\nvector with a 1 in the rth position and zeros elsewhere.\\nAs a result, one can determine the lower α-percentile of the distribution\\nPr(Rn = r | N1 = n1), denoted by Rn(α).\\nRemark 1 Note that in an [n −n1, n1]-speciﬁed random permutation, the dis-\\ntribution of Rn is independent of the success probability p.\\nAn example is given below to illustrate the method.\\nExample 1 Let n = 5 and n1 = 3. Then, n2 = 2 and d = min(n1, n2 + 1) = 3.\\nSuppose a realization of the Bernoulli sequence is 10101; in this case, the\\nnumber of success runs is R5 = 3. For another realization, say 01110, we have\\nR5 = 1.\\nTo compute, for example, Pr(R5 = 2 | N1 = 3), we deﬁne the state space\\nΩ1 = {1, 2, 3}, the initial distribution ξ0 = (1, 0, 0), and the unit vector e2 =\\n(0, 1, 0)⊤. The transition matrices are given by:\\nM1\\n1 =\\n\\uf8ee\\n\\uf8f0\\n0.5 0.5 0\\n0\\n1 0\\n0\\n0 1\\n\\uf8f9\\n\\uf8fb,\\nM1\\n2 =\\n\\uf8ee\\n\\uf8f0\\n0.6 0.4 0\\n0 0.8 0.2\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fb.\\nThen, it follows from Equation (2) that\\nPr(R5 = 2 | N1 = 3) = ξ0\\n 2\\nY\\nt=1\\nM1\\nt\\n!\\ne2 = 0.6.\\n2.2 Scan Statistics\\nThe scan statistic is deﬁned as\\nSn(r) =\\nmax\\n1≤t≤n−r+1 Sn(r, t),\\n(3)\\nwhere Sn(r, t) = Pt+r−1\\ni=t\\nXi, and r is the window size. Fu et al (2012) em-\\nployed the ﬁnite Markov chain imbedding (FMCI) technique to derive the\\nexact conditional distributions of scan statistics.\\nConsider a sequence of Bernoulli trials with binary outcomes {0, 1}. Let\\nΛ = SL\\ni=1 Λi represent a compound pattern composed of L simple patterns\\nΛ1, Λ2, . . . , ΛL, where each simple pattern Λi is a ﬁxed sequence over the\\nbinary alphabet {0, 1} with a speciﬁed length. When any of the simple patterns\\noccurs, the compound pattern is said to occur. Let W(Λ) denote the waiting\\ntime until the ﬁrst occurrence of the compound pattern Λ in the sequence. The\\n6\\nTung-Lung Wu\\ndistribution of the scan statistic is then determined using the FMCI technique\\nby using the dual relationship between the distribution of the scan statistic\\nand the waiting time distribution of the associated compound pattern. An\\nexample is given below.\\nExample 2 Let r = 5 and s = 2. Thus, the event {Sn(5) < 2} means that\\nin every window of ﬁve consecutive trials, there is at most one success. This\\ncorrespondence reﬂects a duality between the scan statistic and local pattern\\noccurrences. Speciﬁcally, the event {Sn(5) < 2} means that every window\\nof length 5 contains at most one success, which is equivalent to requiring\\nthat no two 1s appear within fewer than 5 positions of each other. Hence,\\nthe sequence must avoid all the patterns {11}, {101}, {1001}, and {10001}.\\nThese four patterns form the compound pattern Λ5,2, so that the probability\\nPr(Sn(5) < 2) coincides with the probability that none of the patterns in Λ5,2\\noccurs within the ﬁrst n trials, i.e.,\\nPr(Sn(5) < 2) = Pr(W(Λ5,2) > n),\\nwhere W(Λ5,2) denotes the waiting time until the ﬁrst occurrence of any pat-\\ntern in Λ5,2. In the conditional case, the dual relationship between the scan\\nstatistic and the compound-pattern waiting time still holds. Speciﬁcally,\\nPr (Sn(5) < 2 | N1 = n1) = Pr (W(Λ5,2) > n | N1 = n1) .\\nGiven r and s, a set of simple patterns of lengths no longer than r is deﬁned\\nas\\nΛr,s = {Λi : i = 1, . . . , ℓ},\\nwhere each Λi is a simple pattern that begins and ends with 1 and contains a\\ntotal of s ones, and ℓis the total number of simple patterns corresponding to\\nthe scan statistic with parameters r and s. For each scan statistic probability\\nPr(Sn(r) < s), the total number of simple patterns is\\nℓ=\\nr−s\\nX\\nν=0\\n\\x12s −2 + ν\\nν\\n\\x13\\n.\\nLet\\nP =\\n(\\nπ = (π1, . . . , πn) : πi ∈{0, 1} and\\nn\\nX\\ni=1\\nπi = n1\\n)\\nbe the family of random permutations with n1 ones and n−n1 zeros. Then, the\\nconditional distributions of runs and patterns, given the total number n1 of\\nsuccesses in a sequence of n Bernoulli trials, are the same as the distributions\\nof runs and patterns in an [n −n1, n1]-speciﬁed random permutation π =\\n(π1, . . . , πn). It is notable that, in an [n−n1, n1]-speciﬁed random permutation,\\nthe distributions of runs and patterns are independent of p.\\nTo construct the imbedded Markov chain, a similar insertion process is\\nused and explained as follows. To represent the sequential sampling process,\\nTitle Suppressed Due to Excessive Length\\n7\\nimagine an urn containing n1 balls labeled 1 and n0 = n −n1 balls labeled 0.\\nBalls are drawn one by one without replacement until Λr,s occurs or the urn is\\nemptied. Each draw updates the partial sequence, and we record the current\\nnumber of observed 1s together with the longest subpattern that has appeared\\nso far. Let Er,s denote the collection of all subpatterns of Λr,s (excluding the\\npatterns themselves), known as ending blocks, and let S2 = {0, 1}. Then, the\\nnonhomogeneous Markov chain {Yt}n\\nt=0 can be deﬁned on the state space\\nΩ2 = {(m, ω) : m = 0, 1, . . ., n1 and ω ∈Er,s ∪S2} ∪{∅, α},\\nwhere ∅is the initial state and α is the absorbing state, meaning that the\\ncompound pattern Λr,s occurs once the chain enters the absorbing state.\\nTo make the transition probabilities in Equation (4) fully explicit, we ﬁrst\\nﬁx notation for the imbedded Markov chain {Yt}n\\nt=0. Let π = (π1, . . . , πn)\\ndenote an [n −n1, n1]–speciﬁed random permutation, where each πt ∈{0, 1}\\nrepresents the outcome at time t. For t = 0, 1, . . . , n, deﬁne\\nmt =\\nt\\nX\\ni=1\\nπi,\\nm0 = 0,\\nas the cumulative number of ones observed up to time t. Recall that Er,s is the\\nset of all nonabsorbing subpatterns and S2 = {0, 1}; we write ωt ∈Er,s∪S2 for\\nthe current subpattern after observing πt. For two strings ω and b ∈{0, 1}, we\\ndeﬁne the concatenation operator ⟨ω, b⟩as the word obtained by appending b\\nto ω, and let\\n⟨ω, b⟩Er,s\\ndenote the longest subpattern of this concatenation that belongs to Er,s (or the\\nabsorbing state α if the concatenation completes a compound pattern). With\\nthese deﬁnitions, the three cases in Equation (4) correspond respectively to\\n(i) observing a 1 and incrementing the count mt = mt−1 + 1 while updating\\nthe subpattern to the corresponding longest subpattern; (ii) observing a 0 and\\nkeeping the count unchanged while updating the subpattern; and (iii) staying\\nin the absorbing state α once it has been entered.\\nLet ℓn1+1 denote the size of the state space Ω2. The transition probabilities\\nfrom state u = (mt−1, ωt−1) to state v = (mt, ωt) are\\npuv(t) = Pr(Yt = (mt, ωt)|Yt−1 = (mt−1, ωt−1)),\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nn1−mt−1\\nn−t+1\\nif πt = 1, mt = mt−1 + 1 and ωt =< ωt−1, 1 >Er,s,\\nn−n1−t+mt−1+1\\nn−t+1\\nif πt = 0, mt = mt−1 and ωt =< ωt−1, 0 >Er,s,\\n1\\nif ωt = ωt−1 = α,\\n0\\nif otherwise.\\n(4)\\nTherefore, the transition matrices are of the form\\nM2\\nt =\\n\\x14\\nN2\\nt\\nC2\\nt\\n0\\n1\\n\\x15\\n(ℓn1+1)×(ℓn1+1)\\n,\\n(5)\\n8\\nTung-Lung Wu\\nwhere N2\\nt, t = 1, 2, . . . , are ℓn1 × ℓn1 submatrix of transition probabilities\\namong transient states, and C2\\nt is an ℓn1 × 1 column vector whose ith entry\\ngives the probability that the chain moves from state i into the absorbing\\nstate α.\\nSince we are interested in the event that the waiting time exceeds n (i.e.,\\nthe chain remains within the transient states associated with N2\\nt up to time n),\\nthe conditional waiting-time probability can be obtained from Theorem 2.1 of\\nFu and Lou (2003). Hence, the conditional distribution of Sn(r) given N1 = n1\\nis\\nPr(Sn(r) < s | N1 = n1) = Pr(W(Λr,s) > n | N1 = n1)\\n= ξ0\\n \\nn\\nY\\nt=1\\nN2\\nt\\n!\\n1⊤,\\n(6)\\nwhere ξ0 is a 1×ℓn1 vector of initial probabilities, N2\\nt, t = 1, . . . , n, are ℓn1×ℓn1\\nmatrices whose entries are given in Equation (4), and 1 is a ℓn1 × 1 row vector\\nof ones.\\nThe conditional distribution in Equation (6) is independent of p. For more\\ndetails on the imbedding Markov chain, refer to Fu et al (2012).\\nRemark 2 The two statistics, the number of success runs Rn and the scan\\nstatistic Sn(r), are used for illustrative purposes. In fact, the methodology\\nintroduced in this section can be used to compute the conditional distributions\\nof general runs and patterns of ﬁxed lengths. The construction of the transition\\nmatrix depends on the speciﬁc runs or patterns employed. Further details can\\nbe found in Wu (2020).\\n3 The proposed control charts\\nIn this section, we propose Phase-I distribution-free runs- and patterns-type\\ncontrol charts that are applicable to any underlying process distribution F0.\\nThe methodology for one-sided control charts, as outlined throughout this\\npaper, is designed primarily for detecting upward location shifts. However,\\nit can also be extended to control charts with both lower and upper limits,\\nenabling the detection of both downward and upward shifts.\\nLet {Y1, . . . , Yn} be a sample from an unknown distribution F0. In the\\none-sided control chart, a label of “1” is assigned to an observation if its value\\nexceeds a certain threshold c; otherwise, it is labeled as “0”. Speciﬁcally, we\\ndeﬁne Xn as follows:\\nXn =\\n\\x1a1 if Yn ≥c,\\n0 if Yn < c.\\n(7)\\nThe number of success runs and scan statistics can then be deﬁned for the\\nsequence {Xn}, consisting of the two outcomes 0 and 1. It has been widely es-\\ntablished that runs rules can enhance the detection of small shifts in Shewhart-\\ntype control charts. In addition to the two run statistics Rn and Ln discussed\\nTitle Suppressed Due to Excessive Length\\n9\\nherein, one can consider general runs and patterns rules, denoted by R(k, r, Z),\\nas proposed by Shmueli and Cohen (2003). These rules are deﬁned as follows:\\nif the k of the last r-tested points fall in the region Z, the control chart signals\\nan out-of-control (OC) alert. R(k, r, (c, ∞)) and R(k, k, (c, ∞)) are generally\\nreferred as scan rules and run rules, respectively.\\nGiven a pre-speciﬁed level α, the charting procedure based on the number\\nof success runs Rn (denoted as R-1) is outlined below.\\nAlgorithm 1 The R-1 Control Chart\\n1: Initialization: {Y1, . . . , Yn}, α, n, p0\\n2: Determine the threshold c using Equation (8)\\n3: Obtain {X1, . . . , Xn} using Equation (7) and set n1 = Pn\\ni=1 Xi\\n4: Compute the lower α-percentile Rn(α) using Equation (2)\\n5: Compute the observed value of Rn\\n6: Signal an alarm if Rn ≤Rn(α)\\nSimilarly, the charting procedure based on the scan statistic Sn(r) (denoted\\nas R-2) is given in Algorithm 2.\\nAlgorithm 2 The R-2 Control Chart\\n1: Initialization: {Y1, . . . , Yn}, α, n, p0\\n2: Determine the threshold c using Equation (8)\\n3: Obtain {X1, . . . , Xn} as deﬁned in Equation (7) and set n1 = Pn\\ni=1 Xi\\n4: Compute the upper α-percentile Sn(r, α) using Equation (6)\\n5: Compute the observed value of Sn(r)\\n6: Signal an alarm if Sn(r) ≥Sn(r, α)\\nNote that the runs- and patterns-type statistics are discrete, and therefore,\\ncontrolling the IC signal probability at any given level may not always be fea-\\nsible. As a result, a randomized test is used to control the IC signal probability\\nat the desired level.\\n3.1 Choice of c\\nThe binary transformation in Equation (7),\\nXi = I(Yi ≥c),\\ni = 1, . . . , n,\\nconverts the observed data into a 0–1 sequence, where N1 = Pn\\ni=1 Xi denotes\\nthe number of 1s and N0 = n −N1 the number of 0s. The in-control signal\\nprobability, conditional on N1 = n1, does not depend on the underlying distri-\\nbution F0 or the threshold c, since the conditional distribution of the binary\\nsequence is uniform over all permutations of n1 ones and n0 zeros. Because\\n10\\nTung-Lung Wu\\nthe conditional distribution of the proposed statistic depends only on N1, the\\nchoice of the threshold c determines the baseline proportion\\np0 = Pr(Y ≥c)\\n(8)\\nand therefore inﬂuences the sensitivity of the resulting chart.\\nThe threshold c may generate an excessively sparse or excessively dense se-\\nquence, leading to long strings of zeros or ones and consequently reducing the\\nresolution of the chart. To mitigate this, c is chosen so that the transformed\\nobservations yield a balanced proportion of zeros and ones. In practice, this is\\naccomplished by selecting c as the empirical quantile bq 1−p0 from the Phase I\\nreference sample, where p0 is restricted to a moderate range such as (0.1, 0.8).\\nThis approach prevents degeneracy in the resulting binary sequence. The rec-\\nommended values of p0 are discussed in Section 4.\\n3.2 Identiﬁcation of potential sources of process instability\\nWhen the chart signals in Phase I, we report explicit localization outputs so\\nthat the practitioner can identify where instability occurs.\\nScan statistic (window localization). Let Xi = I(Yi ≥c) and ﬁx the window\\nlength r. For each window Ij = {j, . . . , j + r −1}, deﬁne the count Cj =\\nP\\ni∈Ij Yi. Upon a signal, we return the top-K windows\\nIˆ\\uf6be1, Iˆ\\uf6be2, . . . , Iˆ\\uf6beK such that Cˆ\\uf6be1 ≥Cˆ\\uf6be2 ≥· · · ≥Cˆ\\uf6beK,\\ntogether with their conditional p-values computed under the distribution-free\\nmodel conditional on N1. The center of Iˆ\\uf6be1 is reported as an anomaly location\\nproxy ˆτ. We list the time indices within each of the top-K windows to support\\nroot-cause investigation.\\nSuccess-run statistic (run localization). Let R1 ≥R2 ≥· · · denote the ordered\\nlengths of consecutive runs of 1s in {Xi}. Upon a signal, we return the indices\\nof the longest (or top-K longest) runs, their start and end positions, and con-\\nditional p-values. These segments pinpoint locations where extended success\\nepisodes arise.\\n.\\n4 Numerical Results and Comparison\\nThe proposed control chart is compared with existing ones based on a widely\\nadopted change-point model (Zhou et al 2009; Parpoula 2021). The process\\nmonitoring in Phase-I is analogous to the change-point detection problem,\\nspeciﬁcally for detecting a location shift. When the process is IC, we assume\\nthat the individual observations X1, X2, . . . , Xn are sampled independently\\nTitle Suppressed Due to Excessive Length\\n11\\nfrom an unknown distribution F0 with unknown mean µ0 and variance σ2\\n0.\\nWhen the process is OC, we assume the data are drawn from the following\\nmodel:\\nXt =\\n(\\nF0(x),\\nfor t = 1, . . . , τ,\\nF1(x),\\nfor t = τ + 1, . . . ,\\n(9)\\nwhere τ is the unknown change point.\\nThe IC signal probability is set to 0.005, and three distribution functions\\nare considered: the standard normal distribution, the exponential distribution\\nwith a mean of 1, and the t-distribution with 3 degrees of freedom. These\\ndistribution choices cover the cases of symmetric, skewed, and heavy-tailed\\ndistributions.\\nTo better compare the performance of the proposed control charts with\\nexisting Phase-I control charts, we follow the settings in Parpoula (2021),\\nwhere a step change in the process mean occurs when the process becomes\\nOC. Let µj = E(Xj), with µ1 = µ2 = · · · = µτ = µ0 and µτ+1 = µτ+2 =\\n· · · = µn = µ1 = µ0 + δσ0, for an unknown time point τ. Three OC scenarios\\nare considered:\\nScenario I: τ = 10 for n = 50 and τ = 20 for n = 100;\\nScenario II: τ = 25 for n = 50 and τ = 50 for n = 100;\\nScenario III: τ = 40 for n = 50 and τ = 80 for n = 100.\\nThe three scenarios represent distinct patterns: (I) the OC period is longer\\nthan the IC period; (II) the OC period is equal to the IC period; and (III) the\\nOC period is shorter than the IC period.\\nWe ﬁrst examine the eﬀect of the reference proportion p0 (or equivalently,\\nc) on the performance of the proposed control charts. In Phase I applications,\\nthe threshold c should be selected so that an excessive number of successes is\\nprimarily due to the instability. To illustrate this, we adopt a simple quantile-\\ntargeting rule that sets c according to a desired target success rate p0 = 0.2,\\n0.5, or 0.8. Let ˆq1−p0 be the empirical (1−p0) quantile of the Phase I reference\\nsample; we then deﬁne\\nc = ˆq1−p0,\\nˆp0 = 1\\nn\\nn\\nX\\ni=1\\nI(Yi ≥c).\\nThe empirical powers are evaluated under three shift scenarios and across\\nthree underlying distributions. Figures 1–6 display the performance of the pro-\\nposed scan-based control charts, while Figures 7 and 8 illustrate the perfor-\\nmance of the proposed run-based control charts. Across all cases, the proposed\\nmethods maintain the nominal in-control signal probability and demonstrate\\nstable out-of-control (OC) performance under diverse distributional condi-\\ntions.\\nTo preserve adequate resolution in the monitoring statistic, it is important\\nto avoid selecting an excessively small or large reference proportion p0. Ex-\\ntreme choices of p0 make the binary sequence overly sparse or overly dense,\\n12\\nTung-Lung Wu\\nreducing the discriminative power of the chart. The eﬀect of the threshold c\\n(or equivalently, p0) on chart performance can be observed in Figures 1–6 for\\nthe scan-based charts. In general, a smaller baseline proportion p0 is prefer-\\nable for shifts occurring later in the sequence, leading to fewer observed 1s\\nand enhanced sensitivity near the end of the sequence. Conversely, a larger\\np0 is more eﬀective for early location shifts, where a higher proportion of 1s\\npromotes earlier detection of OC signals. In addition, the choice of p0 should\\naccount for the window size r. As r increases, the expected number of 1s per\\nwindow rises, and a larger p0 becomes advantageous to preserve contrast and\\ndetection power. For smaller window sizes, a smaller p0 should be adopted to\\nprevent saturation in local counts.\\nThe inﬂuence of distributional shape is also evident. For symmetric distri-\\nbutions such as the normal, a balanced reference proportion (p0 ≈0.5) yields\\nconsistently high power and stable behavior across shift locations. For right-\\nskewed distributions such as the exponential, a smaller p0 is recommended to\\noﬀset the smaller probability of extreme high values. By contrast, for heavy-\\ntailed distributions such as the t(3) distribution, a larger p0 tends to enhance\\nsensitivity, as more 1s are expected due to tail behavior.\\nWe provide our recommendations for the choice of p0 in Table 1 across the\\nproposed control charts and the three shift scenarios. The values in parentheses\\nare those used for power comparisons with existing methods. Note that, as\\nexplained above, a smaller baseline proportion is required for right-skewed\\ndistributions; therefore, the baseline proportion is adjusted downward by 0.1\\nfor Exp(1).\\nScenario I\\nτ = 10 or 20\\nScenario II\\nτ = 25 or 50\\nScenario III\\nτ = 40 or 80\\nr = 10\\n0.4–0.6 (0.5)\\n0.3–0.5 (0.4)\\n0.1–0.3 (0.2)\\nr = 25\\n0.6–0.8 (0.7)\\n0.4–0.6 (0.5)\\n0.1–0.3 (0.2)\\nr = 40\\n0.6–0.8 (0.7)\\n0.4–0.6 (0.5)\\n0.1–0.3 (0.2)\\nRn\\n0.6–0.8 (0.7)\\n0.4–0.6 (0.5)\\n0.1–0.3 (0.2)\\nTable 1: Recommended baseline proportions p0.\\nTo investigate the performance of our proposed control charts, we follow\\nsettings similar to those in Ning et al (2015). Three existing nonparametric\\nPhase-I control charts are examined for comparison with our proposed charts\\nbased on runs and patterns:\\n1) Mann–Whitney (MW): Hawkins and Deng (2010) proposed using the\\ntwo-sample MW test for detecting location shifts without assuming nor-\\nmality.\\n2) Kolmogorov–Smirnov (KS): Similar to the idea of Hawkins and Deng\\n(2010), Ross and Adams used the two-sample KS test to detect arbitrary\\ndistributional changes.\\n3) Empirical Likelihood Ratio (ELR): Ning et al (2015) introduced a\\nnonparametric Phase-I control chart for monitoring the location parameter,\\nTitle Suppressed Due to Excessive Length\\n13\\n0\\n1\\n2\\n3\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0\\n1\\n2\\n3\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0\\n1\\n2\\n3\\n0\\n0.005\\n0.01\\n0.015\\n0.02\\n0.025\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0\\n1\\n2\\n3\\n0\\n0.05\\n0.1\\n0.15\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\nFig. 1: OC signal probabilities for the R-2 chart with n = 50 and window size\\nr = 10.\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n1\\n2\\n3\\n0\\n0.02\\n0.04\\n0.06\\n0.08\\n0.1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0\\n1\\n2\\n3\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\nFig. 2: OC signal probabilities for the R-2 chart with n = 50 and window size\\nr = 25.\\n14\\nTung-Lung Wu\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n1\\n2\\n3\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\n0\\n1\\n2\\n3\\n0\\n0.005\\n0.01\\n0.015\\n0\\n1\\n2\\n3\\n0\\n0.005\\n0.01\\n0.015\\n0.02\\n0\\n1\\n2\\n3\\n0\\n2\\n4\\n6\\n8\\n10-3\\nFig. 3: OC signal probabilities for the R-2 chart with n = 50 and window size\\nr = 40.\\n0\\n1\\n2\\n3\\n0\\n0.01\\n0.02\\n0.03\\n0.04\\n0\\n1\\n2\\n3\\n0\\n0.01\\n0.02\\n0.03\\n0\\n1\\n2\\n3\\n0\\n0.01\\n0.02\\n0.03\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n1\\n2\\n3\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nFig. 4: OC signal probabilities for the R-2 chart with n = 100 and window\\nsize r = 10.\\nTitle Suppressed Due to Excessive Length\\n15\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0\\n1\\n2\\n3\\n0\\n0.02\\n0.04\\n0.06\\n0.08\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nFig. 5: OC signal probabilities for the R-2 chart with n = 100 and window\\nsize r = 25.\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\nFig. 6: OC signal probabilities for the R-2 chart with n = 100 and window\\nsize r = 40.\\n16\\nTung-Lung Wu\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.02\\n0.04\\n0.06\\n0.08\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.05\\n0.1\\n0.15\\nFig. 7: OC signal probabilities for the R-1 chart with n = 50.\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.05\\n0.1\\n0.15\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\nFig. 8: OC signal probabilities for the R-1 chart with n = 100.\\nbased on the maximum ELR statistic over potential change points. The null\\ndistribution of this statistic is approximated by a Gumbel distribution.\\nTitle Suppressed Due to Excessive Length\\n17\\nRn\\nSn(10)\\nSn(25)\\nSn(40)\\nn = 50\\nn = 100\\nn = 50\\nn = 100\\nn = 50\\nn = 100\\nn = 50\\nn = 100\\nN (0, 1)\\n0.0045\\n0.0055\\n0.0051\\n0.0042\\n0.0043\\n0.0050\\n0.0053\\n0.0044\\nExp(1)\\n0.0057\\n0.0053\\n0.0042\\n0.0046\\n0.0039\\n0.0053\\n0.0054\\n0.0059\\nt(3)\\n0.0055\\n0.0056\\n0.0048\\n0.0045\\n0.0049\\n0.0058\\n0.0055\\n0.0048\\nTable 2: IC signal probabilities.\\nTable 2 presents the simulated IC signal probabilities, demonstrating that\\nthe proposed R-1 and R-2 control charts eﬀectively control the IC signal prob-\\nability at the desired level of 0.005. The OC signal probabilities are provided\\nin Figures 9 and 10 for n = 50 and n = 100, respectively. From both Figures 9\\nand 10, the other three control charts perform better than the proposed con-\\ntrol charts for exponential distributions. However, our control charts perform\\nsatisfactorily for normal distributions and consistently better for heavy-tailed\\nt-distributions. In particular, the OC signal probabilities of the R-1 chart based\\non the statistic Rn tend to be higher when the change point occurs early or\\nmidway through the sample, regardless of whether the sample size is small or\\nlarge. This characteristic is desirable, as it facilitates early detection of the\\nchange point. In contrast, the R-2 charts, which are based on scan statistics,\\ngenerally yield higher OC signal probabilities when applied to heavy-tailed\\ndistributions such as the t-distribution. It is well known that scan statistics\\nperform optimally when the scanning window size matches the actual cluster\\nsize. Therefore, it is unsurprising that the OC signal probabilities for Sn(10),\\nSn(25), and Sn(40) are highest when the change points occur at τ = 40,\\nτ = 25, and τ = 10, respectively. Among these, Sn(25) and Sn(40) generally\\noutperform Sn(10) when the sample size is n = 100.\\n5 An Application\\nWe apply the proposed R-1 and R-2 control charts to the piston ring data\\npresented in Tables 6.3 and 6E.7 of Montgomery (2009). The data in Table\\n6.3 contain 25 samples, each consisting of ﬁve inside diameter measurements\\nof forged automobile engine piston rings. The ¯x and s control charts used in\\nMontgomery (2009) indicate that the process is IC. There are an additional\\n15 samples for the piston ring process in Table 6E.7. Because subsamples were\\noften collected at diﬀerent production times, aggregating all 200 observations\\ninto a single Phase I dataset may be problematic; therefore, we apply the\\ncharts to the 40 sample means to determine whether a process instability is\\npresent.\\nWe set up the control chart with an IC signal probability α = 0.05. The\\ndata exhibit right skewness. The window sizes for the R-2 charts are selected\\nto be 6 and 10. Thus, we choose small p0 = 0.2 for Sn(6), resulting in 8 ones\\nand 32 zeros. A larger window size requires a somewhat larger p0; therefore, we\\nselect p0 = 0.3 for Sn(10), resulting in 12 ones and 28 zeros. The control limits\\n18\\nTung-Lung Wu\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nPower\\nFig. 9: OC signal probabilities for n = 50. The signal probabilities of the MW,\\nKS, and ELR charts are adopted from Ning et al (2015).\\n0\\n1\\n2\\n3\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nPower\\n0\\n1\\n2\\n3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nPower\\nFig. 10: OC signal probabilities for n = 100. The signal probabilities of the\\nMW, KS, and ELR charts are adopted from Ning et al (2015).\\nTitle Suppressed Due to Excessive Length\\n19\\nRn(α)\\nSn(6, α)\\nSn(10, α)\\nα = 0.05\\n4 (0.0202, p0 = 0.2)\\n5 (0.0123, p0 = 0.2)\\n7 (0.0525, p0 = 0.3)\\nTable 3: Control limits of the R-1 and R-2 control charts.\\n(CLs) for the R-1 and R-2 charts are summarized in Table 3. The desired signal\\nprobability may not be attained exactly since scan statistics are discrete. The\\nactual IC signal probabilities are given in parentheses.\\nThe observed values are R40 = 4, S40(6) = 5, and S40(10) = 7. The cor-\\nresponding window counts for the R-2 chart at each time point are shown in\\nFigure 11. Both the R-1 and R-2 charts signal a process mean shift at the 0.05\\nsigniﬁcance level. To further investigate potential sources of process instabil-\\nity, we list the time indices of windows with p-values smaller than 0.1. The\\nscan statistics S40(6) and S40(10) identify I34 ∪I35 = {34, 35, 36, 37, 38, 39, 40}\\n(p-value = 0.0123) and I31 = {31, 32, 33, 34, 35, 36, 37, 38, 39, 40} (p-value =\\n0.0525), respectively. Similarly, the run statistic R40 indicates that the longest\\nsuccess run of 1s has length 4, occurring at t = 37, 38, 39, 40 (p-value = 0.0253).\\nThus, both charts consistently localize the same region as a potential source\\nof process instability.\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nR-2\\nFig. 11: The R-2 chart of the piston ring data.\\n6 Summary and Discussion\\nThis paper proposes a new class of distribution free control charts based on\\nruns and patterns for individual observations. The methodology utilizes the\\n20\\nTung-Lung Wu\\nconditional distributions of these statistics, particularly the number of success\\nruns and the scan statistic, to design control charts that are independent of the\\nunderlying distribution. The proposed approach leverages the ﬁnite Markov\\nchain imbedding (FMCI) technique to obtain exact conditional distributions of\\nthe monitoring statistics given the number of successes in a Bernoulli sequence.\\nThe strength of the proposed charts lies in their distribution-free nature,\\nmeaning that their in-control (IC) performance, such as the IC signal prob-\\nability, does not depend on the underlying distribution F0. This property is\\nespecially advantageous in Phase I analysis, where the objective is to assess\\nhistorical data to determine whether the process was IC without assuming a\\nspeciﬁc parametric form.\\nTo implement the proposed method, the original observations are trans-\\nformed into a Bernoulli sequence using an appropriate thresholding rule. This\\ntransformation can be based on a known or estimated reference distribution\\nF0, allowing the resulting sequence to be analyzed using runs- and patterns-\\ntype statistics. Among the statistics examined, the number of success runs and\\nscan statistics were studied in detail, and their exact conditional distributions\\nwere derived using the FMCI technique. These conditional distributions were\\nthen employed to determine control limits for the proposed charts.\\nSeveral simulation studies were conducted to evaluate both in-control (IC)\\nand out-of-control (OC) performance. The results demonstrate that the pro-\\nposed charts maintain their nominal IC signal probability precisely and ex-\\nhibit strong detection power across a range of shift scenarios for heavy-tailed\\ndistributions, where traditional parametric charts tend to be unreliable. Per-\\nformance for skewed distributions was comparatively weaker for the proposed\\nand existing charts.\\nThe selection of the baseline proportion p0, which determines the threshold\\nc and thereby the balance of zeros and ones in the binary sequence, was also\\ninvestigated. Simulation results indicate that moderate values of p0 (typically\\nbetween 0.2 and 0.7) provide stable performance across symmetric distribu-\\ntions and heavy-tailed distributions, while smaller p0 values are preferable for\\nright-skewed distributions such as the exponential. Recommended p0 values\\nfor diﬀerent window sizes and shift locations are summarized in Table 1 to\\nguide practical implementation.\\nFinally, the proposed R-1 and R-2 control charts were applied to the well-\\nknown piston ring data to demonstrate their practical utility. Both charts\\neﬀectively detected a potential location shift. Speciﬁcally, the R-1 chart based\\non the number of success runs identiﬁed an extended sequence of large val-\\nues near the end of the sample, while the R-2 chart based on scan statistics\\ncaptured an early localized deviation in the sequence of sample means. To-\\ngether, both charts provide a clearer diagnostic picture of process instability\\nby pinpointing both the timing and extent of the abnormal region. These ﬁnd-\\nings underscore the robustness and sensitivity of the proposed distribution-free\\ncontrol charts, demonstrating their capability to reveal subtle process shifts\\nwithout relying on parametric assumptions.\\nTitle Suppressed Due to Excessive Length\\n21\\nIn conclusion, the proposed framework provides a robust and ﬂexible ap-\\nproach for Phase I process monitoring. The proposed control charts are truly\\ndistribution-free and ﬂexible; other run or pattern statistics can also be uti-\\nlized, allowing practitioners to select monitoring statistics suited to detecting\\nspeciﬁc types of shifts, such as variance changes, and to construct distribution-\\nfree control charts accordingly.\\nReferences\\nAmin RW, Searcy AJ (1991) A nonparametric exponentially weighted mov-\\ning average control scheme. Communications in Statistics - Simulation and\\nComputation 20(4):1049–1072, doi:10.1080/03610919108812996\\nBalakrishnan N, Triantafyllou IS, Koutras MV (2010) A distribution-free con-\\ntrol chart based on order statistics. Communications in Statistics - Theory\\nand Methods 39(20):3652–3677, doi:10.1080/03610920903324858\\nCapizzi G (2015) Recent advances in process monitoring: Nonparametric and\\nvariable-selection methods for phase I and phase II. Quality Engineering\\n27(1):44–67, doi:10.1080/08982112.2015.968046\\nChakraborti S, Eryilmaz S (2007) A nonparametric Shewhart-type signed-rank\\ncontrol chart based on runs. Communications in Statistics - Simulation and\\nComputation 36(2):335–356, doi:10.1080/03610910601158427\\nChakraborti S, Van Der Laan P, Bakir ST (2001) Nonparametric con-\\ntrol charts: an overview and some results. Journal of Quality Technology\\n33(3):304–315, doi:10.1080/00224065.2001.11980081\\nChakraborti S, Eryilmaz S, Human SW (2009) A phase II nonparamet-\\nric control chart based on precedence statistics with runs-type signal-\\ning rules. Computational Statistics & Data Analysis 53(4):1054–1065,\\ndoi:10.1016/j.csda.2008.09.025\\nChamp CW, Woodall WH (1987) Exact results for Shewhart control\\ncharts\\nwith\\nsupplementary\\nruns\\nrules.\\nTechnometrics\\n29(4):393–399,\\ndoi:10.1080/00401706.1987.10488266\\nChatterjee S, Qiu P (2009) Distribution-free cumulative sum control charts\\nusing bootstrap-based control limits. The Annals of Applied Statistics\\n3(1):349–369, doi:10.1214/08-AOAS197\\nChen N, Zi X, Zou C (2016) A distribution-free multivariate control chart.\\nTechnometrics 58(4):448–459, doi:10.1080/00401706.2015.1049750\\nChowdhury S, Mukherjee A, Chakraborti S (2015) Distribution-free phase II\\nCUSUM control chart for joint monitoring of location and scale. Quality and\\nReliability Engineering International 31(1):135–151, doi:10.1002/qre.1677\\nFu JC, Koutras MV (1994) Distribution theory of runs: a Markov chain ap-\\nproach. Journal of the American Statistical Association 89(427):1050–1058,\\ndoi:10.1080/01621459.1994.10476841\\nFu JC, Lou WYW (2003) Distribution theory of runs and patterns and its\\napplications. World Scientiﬁc, Singapore, doi:10.1142/4669\\n22\\nTung-Lung Wu\\nFu JC, Spiring FA, Xie H (2002) On the average run lengths of quality control\\nschemes using a Markov chain approach. Statistics & Probability Letters\\n56(4):369 – 380, doi:10.1016/S0167-7152(01)00183-3\\nFu JC, Shmueli G, Chang YM (2003) A uniﬁed chain approach for\\ncomputing the run length distribution in control charts with simple\\nor compound rules. Statistics & Probability Letters 65(4):457 – 466,\\ndoi:10.1016/j.spl.2003.10.004\\nFu JC, Wu TL, Lou WYW (2012) Continuous, discrete, and condi-\\ntional scan statistics. Journal of Applied\\nProbability 49(1):199–209,\\ndoi:10.1239/jap/1331216842\\nHawkins\\nDM,\\nDeng\\nQ\\n(2010)\\nA\\nnonparametric\\nchange-point\\ncontrol\\nchart.\\nJournal\\nof\\nQuality\\nTechnology\\n42(2):165–173,\\ndoi:10.1080/00224065.2010.11917814\\nJones-Farmer LA, Jordan V, Champ CW (2009) Distribution-free phase i con-\\ntrol charts for subgroup location. Journal of Quality Technology 41(3):304–\\n316, doi:10.1080/00224065.2009.11917784\\nJones-Farmer LA, Ezell JD, Hazen BT (2014) Applying control chart\\nmethods\\nto\\nenhance\\ndata\\nquality.\\nTechnometrics\\n56(1):29–41, URL\\nhttp://www.jstor.org/stable/24587271\\nKoutras MV, Bersimis S, Maravelakis PE (2007) Statistical process con-\\ntrol\\nusing\\nShewhart\\ncontrol\\ncharts\\nwith\\nsupplementary\\nruns\\nrules.\\nMethodology\\nand\\nComputing\\nin\\nApplied\\nProbability\\n9(2):207–224,\\ndoi:10.1007/s11009-007-9016-8\\nLi Z, Qiu P, Chatterjee S, Wang\\nZ (2013) Using p values to de-\\nsign statistical process control charts. Statistical Papers 54(2):523–539,\\ndoi:10.1007/s00362-012-0447-0\\nLou WYW (1997) An application of the method of ﬁnite Markov chain\\nimbedding to runs tests. Statistics & Probability Letters 31(3):155–161,\\ndoi:10.1016/S0167-7152(96)00027-2\\nMontgomery D (2009) Introduction to statistical quality control, 6th edn. Wi-\\nley, Hoboken, NJ\\nNing W, Yeh AB, Wu X, Wang B (2015) A nonparametric phase I control chart\\nfor individual observations based on empirical likelihood ratio. Quality and\\nReliability Engineering International 31(1):37–55, doi:10.1002/qre.1641\\nParpoula C (2021) Phase I non-parametric control charts for individual ob-\\nservations: a selective review and some results. In: Dimotikalis Y, Kara-\\ngrigoriou A, Parpoula C, Skiadas CH (eds) Applied Modeling Tech-\\nniques and Data Analysis 1, Wiley, Hoboken, NJ, chap 13, pp 233–248,\\ndoi:10.1002/9781119821588.ch13\\nShmueli G, Cohen A (2003) Run-length distribution for control charts with\\nruns and scans rules. Communications in Statistics - Theory and Methods\\n32(2):475–495, doi:10.1081/STA-120018196\\nWu TL (2020) Conditional waiting time distributions of runs and patterns\\nand their applications. Annals of the Institute of Statistical Mathematics\\n72(2):531–543, doi:10.1007/s10463-018-0696-3\\nTitle Suppressed Due to Excessive Length\\n23\\nZhou\\nC,\\nZou\\nC,\\nZhang\\nY,\\nWang\\nZ\\n(2009)\\nNonparametric\\ncontrol\\nchart\\nbased\\non\\nchange-point\\nmodel.\\nStatistical\\nPapers\\n50(1):13–28,\\ndoi:10.1007/s00362-007-0054-7\\nZou\\nC,\\nTsung\\nF\\n(2010)\\nLikelihood\\nratio-based\\ndistribution-free\\nEWMA control charts. Journal of Quality Technology 42(2):174–196,\\ndoi:10.1080/00224065.2010.11917815\\n')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test4 = ArxivLoader(query=\"2511.13672\")\n",
    "test4.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1fbf640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "313fecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from Wikipedia) (4.14.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from Wikipedia) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (2025.10.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from beautifulsoup4->Wikipedia) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in f:\\data\\1jupyter\\eagle\\.venv\\lib\\site-packages (from beautifulsoup4->Wikipedia) (4.15.0)\n",
      "Building wheels for collected packages: Wikipedia\n",
      "  Building wheel for Wikipedia (pyproject.toml): started\n",
      "  Building wheel for Wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for Wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11785 sha256=4b095838d2f1e4b5c19bb0ba2ebd0db6030db768cbd980c0600c6056c51e7a6e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\63\\47\\7c\\a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built Wikipedia\n",
      "Installing collected packages: Wikipedia\n",
      "Successfully installed Wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf6c75c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'LangChain', 'summary': \"LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/LangChain'}, page_content='LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n== History ==\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\\nIn February 2024 LangChain released LangSmith, a closed-source observability and evaluation platform for LLM applications, and announced a US $25 million Series A led by Sequoia Capital. On 14 May 2025 the company launched LangGraph Platform into general availability, providing managed infrastructure for deploying long-running, stateful AI agents.\\n\\n\\n== Capabilities ==\\nLangChain\\'s developers highlight the framework\\'s applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\n\\n== LangChain tools ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website\\nDiscord server support hub\\nLangchain-ai on GitHub'),\n",
       " Document(metadata={'title': 'Model Context Protocol', 'summary': 'The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence (AI) systems like large language models (LLMs) integrate and share data with external tools, systems, and data sources. MCP provides a universal interface for reading files, executing functions, and handling contextual prompts. Following its announcement, the protocol was adopted by major AI providers, including OpenAI and Google DeepMind.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Model_Context_Protocol'}, page_content='The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence (AI) systems like large language models (LLMs) integrate and share data with external tools, systems, and data sources. MCP provides a universal interface for reading files, executing functions, and handling contextual prompts. Following its announcement, the protocol was adopted by major AI providers, including OpenAI and Google DeepMind.\\n\\n\\n== Background ==\\nMCP was announced by Anthropic in November 2024 as an open standard for connecting AI assistants to data systems such as content repositories, business management tools, and development environments. The protocol was created at Anthropic by developers David Soria Parra and Justin Spahr-Summers.\\nMCP aims to address the challenge of information silos and legacy systems. Before MCP, developers often had to build custom connectors for each data source or tool, resulting in what Anthropic described as an \"N×M\" data integration problem.\\nEarlier stop-gap approaches—such as OpenAI\\'s 2023 \"function-calling\" API and the ChatGPT plug-in framework—solved similar problems but required vendor-specific connectors. MCP\\'s authors note that the protocol deliberately re-uses the message-flow ideas of the Language Server Protocol (LSP) and is transported over JSON-RPC 2.0. MCP formally specifies stdio and HTTP (optionally with SSE) as its standard transport mechanisms.\\n\\n\\n== Features ==\\nMCP defines a standardized framework for integrating AI systems with external data sources and tools. It includes specifications for data ingestion and transformation, contextual metadata tagging, and AI interoperability across different platforms. The protocol also supports secure, bidirectional connections between data sources and AI-powered tools.\\nMCP enables developers to expose their data via MCP servers or to develop AI applications—referred to as MCP clients—that connect to these servers. Key components of the protocol include a formal protocol specification and software development kits (SDKs), local MCP server support in Claude Desktop applications, and an open-source repository of MCP server implementations.\\n\\n\\n== Applications ==\\nIn the field of natural language data access, MCP enables applications such as AI2SQL to bridge language models with structured databases, allowing plain-language queries.\\nThe protocol is used in AI-assisted software development tools. Integrated development environments (IDEs), coding platforms such as Replit, and code intelligence tools like Sourcegraph have adopted MCP to grant AI coding assistants real-time access to project context.\\n\\n\\n== Implementation ==\\nThe protocol was released with software development kits (SDKs) in programming languages including Python, TypeScript, C# and Java. Anthropic maintains an open-source repository of reference MCP server implementations for popular enterprise systems including Google Drive, Slack, GitHub, Git, Postgres, Puppeteer and Stripe. Developers can create custom MCP servers to connect proprietary systems or specialized data sources to AI systems. There are also design patterns being actively discussed in the community such as \"Code Mode\" or \"Progressive Discovery\".\\n\\n\\n== Adoption ==\\nIn March 2025, OpenAI officially adopted the MCP, following a decision to integrate the standard across its products, including the ChatGPT desktop app, OpenAI\\'s Agents SDK, and the Responses API.\\nMCP can be integrated with Microsoft Semantic Kernel, and Azure OpenAI. MCP servers can be deployed to Cloudflare.\\nDemis Hassabis, CEO of Google DeepMind, confirmed in April 2025 MCP support in the upcoming Gemini models and related infrastructure.\\n\\n\\n== Reception ==\\nThe Verge reported that MCP addresses a growing demand for AI agents that are contextually aware and capable of securely pulling from diverse sources. The protocol\\'s rapid uptake by OpenAI, Google DeepMind, and toolmakers like Zed and Sou')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test5 = WikipediaLoader(query=\"LangChain\", load_max_docs=2)\n",
    "test5.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6562c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
